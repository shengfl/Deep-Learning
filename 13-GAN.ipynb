{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024c8843-bc81-4480-a02b-9b36b1803ffb",
   "metadata": {},
   "source": [
    "## 1. 生成对抗网络（Generative Adversarial Network, GAN）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76b45a-e89b-4a8f-9685-be811555869d",
   "metadata": {},
   "source": [
    "2014年由Ian Goodfellow等人提出，论文地址[GAN](https://arxiv.org/pdf/1711.00937)\n",
    "\n",
    "在博弈论的思维框架下，通过两个神经网络（生成器和判别器）的对抗训练，模拟了“伪造者 vs 鉴定师”的对抗过程，使生成器能够生成逼真的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e06de8-114d-4b69-9f03-41942c1e5fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://production-media.paperswithcode.com/methods/gan.jpeg\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "url = 'https://production-media.paperswithcode.com/methods/gan.jpeg'\n",
    "display(Image(url=url, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb40a43-31d9-479d-ba3f-aac69fc859aa",
   "metadata": {},
   "source": [
    "## 2. 数学表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480fd7d-e89c-413d-86fe-2a3867763610",
   "metadata": {},
   "source": [
    "GAN的训练是轮着进行的  \n",
    "采用的都是概率建模的方法，具有隐状态空间z  \n",
    "由于轮换对称性，KL散度是不对称的，所以将KL对称化，成为JS散度，定义：$D_{JS}=\\frac{1}{2}(D_{KL}(P||\\frac{P+Q}{2})+D_{KL}(Q||\\frac{P+Q}{2}))$   \n",
    "但如果两个概率分布距离很远，重叠区域很小，JS散度值是一个常数，这就意味着没有梯度。  \n",
    "所以进一步的采用沃瑟斯坦距离（Wasserstein distance），又被称为推土机距离。该距离等价于一个推土机规划问题的极值。定义为：\n",
    "$$W_p(\\mu,\\nu):=\\left(\\mathop{\\inf}_{\\gamma\\in\\Gamma(x,y)}\\int d(x,y)^p d\\gamma(x,y)\\right)^{1/p}$$\n",
    "当然，在高维空间中计算这个距离还是比较困难，因为相当于一个规模很大的线性规划问题。另一方法是通过一个神经网络来迫近这个距离。  \n",
    "\n",
    "采用W距离，GAN的优化目标为：\n",
    "$$\\mathop{\\min}_G \\mathop{\\max}_{D\\in 1-Lipschitz}\\mathbb{E}_{x\\sim p_data}[D(x)]-\\mathbb{E}_{z\\sim p_z}[D(G(z))]$$\n",
    "判别器$D(x)$是一个满足1阶Lipschitz条件的函数，即$$|f(x_2)-f(x_1)|\\leqslant 1\\cdot |x_2-x_1|$$\n",
    "这个条件常用来限制神经网络权重的梯度，避免梯度爆炸或消失。 参见d2l [8.5.5. 梯度裁剪](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbdcb6-37c2-4620-935d-ff54b9f712be",
   "metadata": {},
   "source": [
    "后续变体：\n",
    "\n",
    "DCGAN：首个将卷积网络引入GAN，提升图像生成质量。[DCGAN](https://arxiv.org/pdf/1511.06434)\n",
    "\n",
    "Conditional GAN（cGAN）：通过条件标签控制生成内容（如指定生成“猫”的图像）。[Conditional GAN（cGAN）](https://arxiv.org/pdf/1411.1784)\n",
    "\n",
    "ProGAN：渐进式训练，从低分辨率逐步提升至高清图像。[ProGAN](https://arxiv.org/pdf/1710.10196)\n",
    "\n",
    "BigGAN：大规模训练生成高多样性、高分辨率图像。[BigGAN](https://arxiv.org/pdf/1809.11096)\n",
    "\n",
    "StyleGAN：通过风格控制生成细节丰富的图像（如人脸毛孔、发丝）。[StyleGAN](https://arxiv.org/pdf/1812.04948)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc60fa-a14d-4d13-9e3f-6c8941275cb5",
   "metadata": {},
   "source": [
    "## 3.简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "887b210c-9535-47ab-959b-c3ab65161bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e85d8d2-fd80-480f-a52a-018d65c4a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "batch_size = 64\n",
    "latent_dim = 128\n",
    "epochs = 30\n",
    "lr_D = 1e-4\n",
    "lr_G = 1e-4\n",
    "lambda_gp = 10\n",
    "n_critic = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nz = 100  # 噪声向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19213d55-237a-44b8-86ea-2eb1b768f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 CelebA 数据集加载类\n",
    "class CelebAFromFolder(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted([\n",
    "            fname for fname in os.listdir(image_dir)\n",
    "            if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # 返回0作为占位标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "008f8542-f39a-4300-b2c0-23d4d3ed0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5068, 0.4254, 0.3832],\n",
    "                         std=[0.2971, 0.2789, 0.2815])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06048064-b6d2-4a23-b64b-e45d0c7b3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化数据集\n",
    "dataset = CelebAFromFolder('./data/img_align_celeba', transform=transform)\n",
    "# 只取前10000张图像\n",
    "subset_dataset = Subset(dataset, range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c124fa45-cbf7-4361-a054-f80f9fe8e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器\n",
    "train_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4877524-36b1-4e74-8031-05c0727338a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # [B, 64, 32, 32]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # [B, 128, 16, 16]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # [B, 256, 8, 8]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # [B, 512, 4, 4]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1, 4, 1, 0) # [B, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97586b50-6d20-49da-a7d3-78a85bdc14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, 512, 4, 1, 0),  # [B, 512, 4, 4]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # [B, 256, 8, 8]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # [B, 128, 16, 16]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # [B, 64, 32, 32]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # [B, 3, 64, 64]\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67ab7892-37c4-4007-aa15-c42b6bc0181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型和优化器\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(0.0, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06013d92-3415-491b-8e77-ef5fc5c422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(d_interpolates.size(), device=device)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa4d0885-a924-4c2c-a9f4-40967b858e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定噪声用于可视化\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "091edc4c-9785-44fa-b286-faeca8fef4d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][0/157] Loss_D: -11.7737, Loss_G: 6.7276, GP: 0.0091\n",
      "[0/30][100/157] Loss_D: -319.7731, Loss_G: 161.3455, GP: 12.8314\n",
      "[1/30][0/157] Loss_D: -627.5335, Loss_G: 278.2659, GP: 4.1834\n",
      "[1/30][100/157] Loss_D: -1252.0129, Loss_G: 644.4818, GP: 0.9079\n",
      "[2/30][0/157] Loss_D: -43.8690, Loss_G: -455.1148, GP: 2.3239\n",
      "[2/30][100/157] Loss_D: -70.7683, Loss_G: -335.0363, GP: 2.1133\n",
      "[3/30][0/157] Loss_D: -72.4328, Loss_G: -304.3444, GP: 1.3387\n",
      "[3/30][100/157] Loss_D: -287.6983, Loss_G: -25.4743, GP: 5.2499\n",
      "[4/30][0/157] Loss_D: -260.5536, Loss_G: -5.6636, GP: 2.5074\n",
      "[4/30][100/157] Loss_D: -745.1209, Loss_G: 304.5760, GP: 6.3459\n",
      "[5/30][0/157] Loss_D: -458.0331, Loss_G: 24.6500, GP: 11.1673\n",
      "[5/30][100/157] Loss_D: -514.1838, Loss_G: 193.6291, GP: 41.4942\n",
      "[6/30][0/157] Loss_D: -905.1974, Loss_G: 277.9176, GP: 8.7006\n",
      "[6/30][100/157] Loss_D: -1939.9958, Loss_G: 979.3066, GP: 6.4402\n",
      "[7/30][0/157] Loss_D: -1672.5925, Loss_G: 524.4237, GP: 1.8691\n",
      "[7/30][100/157] Loss_D: -2453.7329, Loss_G: 1032.5045, GP: 10.4381\n",
      "[8/30][0/157] Loss_D: -2427.6738, Loss_G: 883.4366, GP: 2.4692\n",
      "[8/30][100/157] Loss_D: -2958.3840, Loss_G: 1122.2925, GP: 17.2325\n",
      "[9/30][0/157] Loss_D: -2251.4883, Loss_G: 1239.9978, GP: 110.6316\n",
      "[9/30][100/157] Loss_D: -4927.3008, Loss_G: 2370.2878, GP: 2.6477\n",
      "[10/30][0/157] Loss_D: -4158.0015, Loss_G: 1551.9285, GP: 7.2565\n",
      "[10/30][100/157] Loss_D: -4515.3511, Loss_G: 1902.0280, GP: 28.7721\n",
      "[11/30][0/157] Loss_D: -5516.7124, Loss_G: 2088.8267, GP: 2.8792\n",
      "[11/30][100/157] Loss_D: -6937.8594, Loss_G: 3461.3877, GP: 12.5474\n",
      "[12/30][0/157] Loss_D: -6490.1948, Loss_G: 2539.8774, GP: 8.1613\n",
      "[12/30][100/157] Loss_D: -8112.6240, Loss_G: 3742.6399, GP: 10.6163\n",
      "[13/30][0/157] Loss_D: -9556.0361, Loss_G: 4569.4585, GP: 1.9576\n",
      "[13/30][100/157] Loss_D: -9876.2520, Loss_G: 4760.1963, GP: 36.7974\n",
      "[14/30][0/157] Loss_D: -10799.3535, Loss_G: 5235.5879, GP: 10.5601\n",
      "[14/30][100/157] Loss_D: -12432.8105, Loss_G: 5957.2061, GP: 10.8035\n",
      "[15/30][0/157] Loss_D: -11255.1436, Loss_G: 4430.1074, GP: 3.5238\n",
      "[15/30][100/157] Loss_D: -12499.3535, Loss_G: 4976.5371, GP: 7.1237\n",
      "[16/30][0/157] Loss_D: -15171.9072, Loss_G: 7209.4839, GP: 6.1228\n",
      "[16/30][100/157] Loss_D: -14372.6768, Loss_G: 5738.1504, GP: 9.2349\n",
      "[17/30][0/157] Loss_D: -15080.4111, Loss_G: 6021.7095, GP: 8.3066\n",
      "[17/30][100/157] Loss_D: -18657.4629, Loss_G: 8878.8496, GP: 9.4077\n",
      "[18/30][0/157] Loss_D: -19884.0977, Loss_G: 9487.7754, GP: 7.2395\n",
      "[18/30][100/157] Loss_D: -20945.7227, Loss_G: 10138.5527, GP: 33.8270\n",
      "[19/30][0/157] Loss_D: -19276.4355, Loss_G: 7695.9512, GP: 7.2385\n",
      "[19/30][100/157] Loss_D: -22420.1250, Loss_G: 8876.7441, GP: 171.5909\n",
      "[20/30][0/157] Loss_D: -20770.7910, Loss_G: 8788.5840, GP: 102.7061\n",
      "[20/30][100/157] Loss_D: -26745.9277, Loss_G: 12772.8164, GP: 8.2102\n",
      "[21/30][0/157] Loss_D: -24983.2832, Loss_G: 10748.9297, GP: 8.7895\n",
      "[21/30][100/157] Loss_D: -29902.4551, Loss_G: 14235.7715, GP: 4.8404\n",
      "[22/30][0/157] Loss_D: -26726.6113, Loss_G: 10713.3418, GP: 16.7401\n",
      "[22/30][100/157] Loss_D: -33097.1953, Loss_G: 15815.2715, GP: 9.3941\n",
      "[23/30][0/157] Loss_D: -29612.4473, Loss_G: 11843.8496, GP: 9.0394\n",
      "[23/30][100/157] Loss_D: -36145.1367, Loss_G: 17366.6758, GP: 8.6931\n",
      "[24/30][0/157] Loss_D: -34228.8086, Loss_G: 15711.5508, GP: 22.3256\n",
      "[24/30][100/157] Loss_D: -39807.4258, Loss_G: 19035.1836, GP: 8.3760\n",
      "[25/30][0/157] Loss_D: -39812.7812, Loss_G: 19625.7656, GP: 109.9223\n",
      "[25/30][100/157] Loss_D: -42890.8359, Loss_G: 20548.6680, GP: 19.3468\n",
      "[26/30][0/157] Loss_D: -37583.1797, Loss_G: 15499.6377, GP: 106.0091\n",
      "[26/30][100/157] Loss_D: -46832.8594, Loss_G: 22408.9629, GP: 9.3473\n",
      "[27/30][0/157] Loss_D: -41621.2031, Loss_G: 16849.6016, GP: 33.0140\n",
      "[27/30][100/157] Loss_D: -50452.1719, Loss_G: 24200.6562, GP: 11.5222\n",
      "[28/30][0/157] Loss_D: -44950.8242, Loss_G: 18095.7109, GP: 23.8730\n",
      "[28/30][100/157] Loss_D: -54482.1797, Loss_G: 26025.6406, GP: 9.3122\n",
      "[29/30][0/157] Loss_D: 4865.0605, Loss_G: 26504.3398, GP: 976.6043\n",
      "[29/30][100/157] Loss_D: -56937.2891, Loss_G: 27591.8672, GP: 79.7079\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        b_size = real_images.size(0)\n",
    "\n",
    "        # 训练判别器\n",
    "        for _ in range(n_critic):\n",
    "            netD.zero_grad()\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake_images = netG(noise).detach()\n",
    "\n",
    "            D_real = netD(real_images)\n",
    "            D_fake = netD(fake_images)\n",
    "\n",
    "            gp = compute_gradient_penalty(netD, real_images.data, fake_images.data)\n",
    "            lossD = -torch.mean(D_real) + torch.mean(D_fake) + lambda_gp * gp\n",
    "            lossD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        netG.zero_grad()\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        lossG = -torch.mean(netD(fake_images))\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{epoch}/{epochs}][{i}/{len(train_loader)}] Loss_D: {lossD.item():.4f}, Loss_G: {lossG.item():.4f}, GP: {gp.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "        utils.save_image(fake, f\"./data/GAN/epoch_{epoch:03d}.png\", normalize=True, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebead11-02d0-4679-8372-aabbace217b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
