{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa467b57-a9f7-41e0-a4a4-71f158a58a75",
   "metadata": {},
   "source": [
    "## 1. Deep RNNs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbe633-3296-4ae7-8c31-b9d021fb07ac",
   "metadata": {},
   "source": [
    "有多个隐藏层的RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9cbcb6-bba8-4f75-9db3-6ede25a0e14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d2l.ai/_images/deep-rnn.svg\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "url = 'https://d2l.ai/_images/deep-rnn.svg'\n",
    "display(Image(url=url, width=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71349dcd-223f-42a1-a3f2-ce603db4f544",
   "metadata": {},
   "source": [
    "## 2. 数学表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f36c0-a264-4ff4-b87b-bfacff034e33",
   "metadata": {},
   "source": [
    "$t$时刻的隐藏输出：$H_t^{(l)} = \\phi(W_{hh}^{(l)}H_{t-1}^{(l)}+W_{xh}^{(l)}H_t^{(l-1)}+b)$\n",
    "\n",
    "特别的，$x_t = H_t^{0}$\n",
    "\n",
    "$t$时刻的预测输出：$\\hat X_t = \\phi(W_{hx}^{(L)}H_{t}^{(L)}+b)$\n",
    "\n",
    "用分块矩阵乘法重新表示下$H_t^{(l)}$：$H_t^{(l)} = \\phi(\\left[\\begin{array}{c|r}W_{hh}^{(l)} & W_{xh}^{(l)}\\end{array}\\right] \\left[\\begin{array}{cc|r}H_{t-1}^{(l)}\\\\H_t^{(l-1)}\\end{array}\\right]+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f915515-5850-43ec-89c0-883783b6d13c",
   "metadata": {},
   "source": [
    "## 3. 文本分类\n",
    "\n",
    "基于以 imdb 数据集，实现了一个基本的 RNN 文本分类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26e761-0439-4ca4-952d-b9849b9a506e",
   "metadata": {},
   "source": [
    "### 3.1 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d00a50f-aa0c-4fed-94a1-ce80233ca50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2TokenizerFast, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6ebfd-b208-4735-ac06-f816ad2d2587",
   "metadata": {},
   "source": [
    "### 3.2 设定超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f3931aab-7ab2-425e-8f37-9578c9889b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset_name\": \"wikitext\",\n",
    "    \"dataset_config\": \"wikitext-2-raw-v1\",\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length\": 64,\n",
    "    \"embed_dim\": 64,\n",
    "    \"hidden_dim\": 32,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb6d51-8eb3-4551-b823-088828a5e05c",
   "metadata": {},
   "source": [
    "### 3.3 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3cabf-5093-42e1-b081-b806edcaf009",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 加载wiki数据集\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_config\u001b[39m\u001b[38;5;124m'\u001b[39m], cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 加载wiki数据集\n",
    "dataset = load_dataset(config['dataset_name'], config['dataset_config'], cache_dir='./data')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "662d54a6-68d3-4c49-9074-da554355fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ''}\n",
      "{'text': ' = Valkyria Chronicles III = \\n'}\n",
      "{'text': ''}\n",
      "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'}\n",
      "{'text': \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"}\n",
      "{'text': \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\"}\n",
      "{'text': ''}\n",
      "{'text': ' = = Gameplay = = \\n'}\n",
      "{'text': ''}\n",
      "{'text': \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bc80c099-95de-4295-b841-1f6f7ccaa89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258\n"
     ]
    }
   ],
   "source": [
    "# 分词器\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(config['model_name'], cache_dir='./cache') \n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72d35f25-1ac2-43d7-b45b-e025e6bd53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 78 ['The', 'ĠHub', 'Ġhas', 'Ġsupport', 'Ġfor', 'Ġdozens', 'Ġof', 'Ġlibraries', 'Ġin', 'Ġthe', 'ĠOpen', 'ĠSource', 'Ġecosystem', '.', 'ĠThanks', 'Ġto', 'Ġthe', 'Ġhugging', 'face', '_', 'hub', 'ĠPython', 'Ġlibrary', ',', 'Ġit', 'âĢ', 'Ļ', 's', 'Ġeasy', 'Ġto', 'Ġenable', 'Ġsharing', 'Ġyour', 'Ġmodels', 'Ġon', 'Ġthe', 'ĠHub', '.', 'ĠThe', 'ĠHub', 'Ġsupports', 'Ġmany', 'Ġlibraries', ',', 'Ġand', 'Ġwe', 'âĢ', 'Ļ', 're', 'Ġworking', 'Ġon', 'Ġexpanding', 'Ġthis', 'Ġsupport', '.', 'ĠWe', 'âĢ', 'Ļ', 're', 'Ġhappy', 'Ġto', 'Ġwelcome', 'Ġto', 'Ġthe', 'ĠHub', 'Ġa', 'Ġset', 'Ġof', 'ĠOpen', 'ĠSource', 'Ġlibraries', 'Ġthat', 'Ġare', 'Ġpushing', 'ĠMachine', 'ĠLearning', 'Ġforward', '.']\n"
     ]
    }
   ],
   "source": [
    "# 示例\n",
    "text = \"The Hub has support for dozens of libraries in the Open Source ecosystem. \\\n",
    "Thanks to the huggingface_hub Python library, it’s easy to enable sharing your models on the Hub. \\\n",
    "The Hub supports many libraries, and we’re working on expanding this support. \\\n",
    "We’re happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.\"\n",
    "\n",
    "# 分词\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3b5bfb2-03d2-4288-a40e-16bc5ce94b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf92cf3af9a4237ac97bdf69829cffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义预处理函数\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(\n",
    "        dataset[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=config[\"seq_length\"],\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=config[\"seq_length\"] // 2,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "# 应用预处理\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ba79cf0-d2b0-4743-bb36-50ca376f9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464, 14699,   468,  1104,   329,  9264,   286, 12782,   287,   262,\n",
      "          4946,  8090, 13187,    13,  6930,   284,   262, 46292,  2550,    62]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9b1d45b8-f897-4498-9c2a-e3f082f2343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 Dataset 类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.input_ids[idx])\n",
    "        return input_ids, input_ids.clone()  # 目标就是输入的下一个 token\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85fa6436-65b0-4c4a-ba81-29141b84d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12168,  6154,   423,  ..., 50256, 50256, 50256],\n",
      "        [  796,   796, 12556,  ..., 50256, 50256, 50256],\n",
      "        [  796,   796,  2159,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [  383,   749,  8018,  ..., 50256, 50256, 50256],\n",
      "        [  554,  9162,   837,  ..., 50256, 50256, 50256]]) tensor([[12168,  6154,   423,  ..., 50256, 50256, 50256],\n",
      "        [  796,   796, 12556,  ..., 50256, 50256, 50256],\n",
      "        [  796,   796,  2159,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [  383,   749,  8018,  ..., 50256, 50256, 50256],\n",
      "        [  554,  9162,   837,  ..., 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    input_ids, target_ids = batch\n",
    "    print(input_ids, target_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225c4f7-599d-4def-b9a1-e16391f58f9d",
   "metadata": {},
   "source": [
    "### 3.4 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a53a57ff-939d-49ac-be8b-5ab55a55a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 RNN 模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942d1b4-c4f7-4916-a6b9-601d47d067ba",
   "metadata": {},
   "source": [
    "### 3.5 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa68050b-9b9f-4cf4-9d50-4e17c1c7e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "model = RNNModel(vocab_size=vocab_size, embed_dim=config[\"embed_dim\"], hidden_dim=config[\"hidden_dim\"], \n",
    "                 num_layers=config[\"num_layers\"], dropout=config[\"dropout\"]).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e265479-9da2-4670-9e5f-59935ef2d476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNNModel                                 --\n",
       "├─Embedding: 1-1                         3,216,512\n",
       "├─RNN: 1-2                               7,360\n",
       "├─Linear: 1-3                            1,658,514\n",
       "├─Dropout: 1-4                           --\n",
       "=================================================================\n",
       "Total params: 4,882,386\n",
       "Trainable params: 4,882,386\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e19683-26fc-4d4d-82b9-acef116dddbd",
   "metadata": {},
   "source": [
    "### 3.6 选择损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b657f6a2-536c-4bc6-87e4-5caee0389757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "scaler = GradScaler()  # 半精度训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab969d-90dc-44d5-987d-0ee313547007",
   "metadata": {},
   "source": [
    "### 3.7 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdab1ff5-1867-4aa9-9d45-277d4344f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(config['device']), target_ids.to(config['device'])\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # 手动管理显存\n",
    "\n",
    "            with autocast(device_type='cuda'):  # 混合精度训练\n",
    "                output = model(input_ids)\n",
    "                loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 释放显存\n",
    "            del input_ids, target_ids, output, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8158f-3945-4e26-a571-9c4436787f69",
   "metadata": {},
   "source": [
    "### 3.8 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13e93ea6-1531-4837-bcb3-9445840d2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 9.2707\n",
      "Epoch [2/20], Loss: 7.4201\n",
      "Epoch [3/20], Loss: 6.9457\n",
      "Epoch [4/20], Loss: 6.7038\n",
      "Epoch [5/20], Loss: 6.5250\n",
      "Epoch [6/20], Loss: 6.3356\n",
      "Epoch [7/20], Loss: 6.2328\n",
      "Epoch [8/20], Loss: 6.1387\n",
      "Epoch [9/20], Loss: 6.0454\n",
      "Epoch [10/20], Loss: 5.9370\n",
      "Epoch [11/20], Loss: 5.8748\n",
      "Epoch [12/20], Loss: 5.8157\n",
      "Epoch [13/20], Loss: 5.7470\n",
      "Epoch [14/20], Loss: 5.6954\n",
      "Epoch [15/20], Loss: 5.6381\n",
      "Epoch [16/20], Loss: 5.5857\n",
      "Epoch [17/20], Loss: 5.5471\n",
      "Epoch [18/20], Loss: 5.5017\n",
      "Epoch [19/20], Loss: 5.4573\n",
      "Epoch [20/20], Loss: 5.4199\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, criterion, scaler, config[\"epochs\"], config[\"device\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
